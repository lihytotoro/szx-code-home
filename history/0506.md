# 0506
抓紧推理出来，并使用 docker 脚本进行测试！

# 推理部分
按照之前的说法，总共进行过 9 组实验的训练（3 cwe-inf + 2 cwe-apr + 4 direct），目前 loss 下降的都还可以
现在的困难在于：
    进行 inference 的时候，原先 repairllama 的版本是自带 10 beams 的，而 Firefly 的 chat 脚本不仅是 beam=1 的，并且没有输出成文件
    在尝试将更多的样例输出之前，先进行一下初步性能测试，看看模型的回复效果究竟如何？
    为此，前几天正在做的事 —— 需要将测试用样例是如何生成的写清楚
    理论上应该位于：repairllama -> lora -> pred 的脚本中

    目标文件路径：/home/lihaoyu/szx/proj/github-proj/repairllama/src/lora/pred/llama_pred.py



# 这里记录一下解决问题过程中搜索到的结果
A. beam_search

在每一个时间步t，都保留多个不同的候选结果，直到最后一步才确定到底哪一个候选结果更为占优。候选结果的数量就是参数"num_beams"
步骤：
k为num beams
    1.选取概率最高的k个词，加入inputs
    2.计算联合分布概率，生成词的概率乘当前词概率，选取最高的k个，加入input
    3.重复，直到k个序列都遇到结束符或者到了最大长度
这里有个trick，即对应每个束(这里有k个) next token，只需要选取top k进行计算

B. Greedy (Maximization)

每次只取概率最大的词，其结果是固定的
步骤：
1.选取概率最高的词，加入input
2.重复，直到遇到结束符eos或者pad符，或者到了最大长度

C. TopK

当要生成下一个词的时候，只截断选取概率最高的前K个词，对这K个词的概率重新进行归一化，然后在其中进行采样
在Top-K 采样中, 依旧是从概率分布中，依据概率最大选择k个单词中，不同的点在于，该方法会对这K个词的概率重新再次进行分布(redistributed)，然后依据新的概率分布重新取下一个token。GPT2模型就是用的这种采样方法

D. TopP (Nucleus sampling)

根据分布的形状灵活地设定 k，从概率最高的词开始构造采样空间，直到采样空间里所有词的概率加和超过阈值 p 停止
对当前的所有词的概率按照从大到小开始累加，当累加的值大于阈值P的时候，后面小的概率词就不使用，对前面的词再进行sampling

与top k对低概率词汇直接丢弃的处理方法不同，top p采用的是累计概率的方式。即从累计概率超过某一个阈值p的词汇中进行采样。换言之，根据参数p的大小调节(0<=p<=1), Top-P Sampling增大了出现概率较小的词汇的生成的概率。

E. Temperature？
emperature sampling的想法源于热力统计学的概念，温度高往往意味着更容易是低能量状态。在概率模型中， logits 代表能量值，将其送入softmax函数，除以temperature值，得到最终的采样概率分布。

使用 “temperature” Reweighting 分布 -> softmax 函数 -> 从 reweighted 分布中，重新选取下一个 token