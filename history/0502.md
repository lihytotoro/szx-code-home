# 这里记录一些关于 lora peft 的笔记
要使用 LoRA， 您需要在 LoraConfig 中指定目标模块，
以便 get_peft_model（） 知道我们模型中的哪些模块需要使用 LoRA 矩阵进行修改.
在此示例中，我们只对定位基本模型的注意力块的查询和值矩阵感兴趣。
由于对应于这些矩阵的参数分别被“named”“query”和“value”，
我们在 LoraConfig 的 target_modules 参数中相应地指定它们.

from peft import LoraConfig, get_peft_model
​
config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["query", "value"],
    lora_dropout=0.1,
    bias="none",
    modules_to_save=["classifier"],
)
lora_model = get_peft_model(model, config)
print_trainable_parameters(lora_model)
"trainable params: 667493 || all params: 86466149 || trainable%: 0.77"

我们希望在自定义数据集上微调基本模型时也训练分类器参数。
为了确保分类器参数也经过训练，我们指定了 modules_to_save。这也确保了在使用 save_pretrained（） 和 push_to_hub（） 等实用程序时，这些模块与 LoRA 可训练参数一起序列化。

与非 PEFT 方法相比，您可以使用更大的批大小，因为要训练的参数更少。您还可以设置比正常学习率更高的学习率（例如 1e-5）。

# 下面的内容似乎有关之前产生过疑惑的 PeftModel.from_pretrained？
我们看看如何加载 LoRA 更新的参数以及我们的基本模型进行推理.
当您使用 PeftModel 包装基本模型时，修改是就地完成的。若要缓解可能因就地修改而产生的任何问题，请像之前一样初始化基本模型并构造推理模型。

from peft import PeftConfig, PeftModel
​
config = PeftConfig.from_pretrained(repo_name)
model = AutoModelForImageClassification.from_pretrained(
    config.base_model_name_or_path,
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint
)
# Load the LoRA model
inference_model = PeftModel.from_pretrained(model, repo_name)

上面的代码先构造了基底模型，然后使用 PeftModel 的方法进行了二次 load，这样加载完的模型可以直接用于推理





# 得出结论？
之前复现 repairllama 时，使用 peftmodel 进行模型二次套壳是没有意义的？也许不至于错但是可以注释掉
目前缺失的功能？
    Firefly 没有写断点重新开始继续训练的功能（比如训练完了 epoch=1，继续训练到 epoch=2~3）
    这项任务必须在今晚之前写完。然后在有卡的情况下，将 lora/qlora 训练 base/instruct 进行 direct-apr 任务到 2~3 个 epoch

# 在此之前要完成的任务？
目前我们先不继续训练更多的模型了，而是先玩转 Firefly 中的 chat 函数
流程：single chat -> 收集回复 -> 写 for 循环，转变为 multi_samples chat，即将所有待测试样例的回复统一收集到一个文件中（possibly jsonl?）

另外，有一个小任务：在现在的模式下，输出 log 的间隔似乎太短了，调小一点